---
title: "Clustering Analysis on the Covid-19 Alignment Hit Table"
author: "Group 3 - CORONAVIRUS"
date: "6/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Preprocessing and Exploration

First, read table from the CSV file.
```{r data_01}
corona <- read.table('MN997409.1-4NY0T82X016-Alignment-HitTable.csv', header = TRUE, sep=',')
summary(corona)
```

Print the structure of the dataset.
```{r data_01_2, echo=FALSE}
str(corona)
```


Inspect the correlation between attributes.
```{r data_02, warning=FALSE}
corona1 <- corona[, -c(1,2)]
m <- cor(corona1)
m
```

Import libraries used for plotting.
```{r data_03, message=FALSE, warning=FALSE}
library(ggplot2)
library(tidyr)
```

Plot histograms to show the distributions for each attribute.
```{r data_05, echo=FALSE, results='hide'}
corona1 %>% gather() %>% head()

histogram <- ggplot(gather(corona1), aes(value)) + 
  geom_histogram(bins = 10) + 
  facet_wrap(~key, scales = 'free_x')

histogram
```

Plot scatter matrix between all pairs of two attributes in the dataset.
```{r data_06, echo=FALSE}
#Plot scatter matrix
scatter <- pairs(corona1)
```

Import "car" library as we will use the function `scatterplot()` to generate scatter plots for highly correlated attributes.
```{r data_07, message=FALSE, warning=FALSE}
library("car")
```

Plot individual scatter plots for pairs of attributes with high correlations. The selected pairs of features are:

- `X.mismatches.` and `X.gap.opens.`
- `X.s..start.` and `X.q..start.`
- `X.s..end.` and `X.q..end. `
- `X.alignment.length.` and `X.bit.score.`

```{r data_08}
scatterplot(X.mismatches. ~ X.gap.opens., data = corona) 
scatterplot(X.s..start. ~ X.q..start., data = corona) 
scatterplot(X.s..end. ~ X.q..end., data = corona) 
scatterplot(X.alignment.length. ~ X.bit.score., data = corona)
```

Next, inspect the data by calculating the lower and upper bounds of each attribute using IQR rule.

```{r data_09}
# Generate lower and upper bounds using IQR rule
# Calculate quartile 1 and 3 for all columns
coronaQuantiles <- sapply(corona1, quantile, na.rm = TRUE)
coronaQ1 <- coronaQuantiles[2,]
coronaQ3 <- coronaQuantiles[4,]

# Calculate lower and upper bounds to detect outlier for all columns
coronaIQR <- sapply(corona1, IQR, na.rm = TRUE)
coronaS = 1.5 * coronaIQR
coronaLB = coronaQ1 - coronaS
coronaUB = coronaQ3 + coronaS

# Show lower and upper bounds for all columns
corona.bounds <- data.frame(lower.bound = coronaLB, 
                            upper.bound = coronaUB)
corona.bounds
```

Display the columns with outlier values below the lower bound according to the IQR rule. 

```{r data_10}
# Print columns and number of outliers below the lower bound (IQR rule).
coronaMin <- coronaQuantiles[1,]

for (i in 1:length(coronaLB)) {
    if (coronaLB[[i]] > coronaMin[[i]]) {
        col_name <- names(coronaLB)[i]
        col_LB <- coronaLB[[i]]
        
        corona1belowLB <- corona1[which(corona1[col_name] < col_LB),][col_name]
        print(paste0(col_name, ": ", nrow(corona1belowLB), " of ", nrow(corona1), " records"))
    }
}
```


Display the columns with outlier values above the upper bound according to the IQR rule.

```{r data_11}
# Print columns and number of outliers above the upper bound (IQR rule).
coronaMax <- coronaQuantiles[5,]

for (i in 1:length(coronaUB)) {
    if (coronaUB[[i]] < coronaMax[[i]]) {
        col_name <- names(coronaUB)[i]
        col_UB <- coronaUB[[i]] 
        
        corona1aboveLB <- corona1[which(corona1[col_name] > col_UB),][col_name]
        print(paste0(col_name, ": ", nrow(corona1aboveLB), " of ", nrow(corona1), " records"))
    }
}
```

---

## Hierarchical Clustering

<!-- You can also embed plots, for example: -->

<!-- ```{r pressure, echo=FALSE} -->
<!-- plot(pressure) -->
<!-- ``` -->

<!-- Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot. -->


### Data Preparation for Hierarchical Clustering

```{r hclust_01}
# Remove "X.query.acc.ver." and "X.evalue."
corona.hclust <- corona[-c(1, 11)]

# Define attributes used for hierarchical clustering
vars.to.use <- colnames(corona.hclust)[-1]   # 9 attributes

# Transform data frame into a scaled matrix
corona_scale <- scale(corona.hclust[,vars.to.use])

# Create distance matrix
d_euclidean <- dist(corona_scale, method = "euclidean")

```

### Perform Hierarchical Clustering + Dendrogram

Define helper functions as follows:

**Helper function #1**: Print selected attributes of the samples in each cluster
```{r hclust_02}
print_clusters <- function(labels, k) { 
  for(i in 1:k) { 
    print(paste("cluster", i)) 
    print(corona[labels==i, c("X.subject.acc.ver.", 
                              "X.bit.score.", 
                              "X.alignment.length.", 
                              "X.mismatches.", 
                              "X.gap.opens.")]) 
  }
}

```

**Helper function #2**: Generate dendrogram, specifying $k$ is optional.
```{r hclust_03}
plot_hclust <- function(pfitHClust, k = NA, label = FALSE) {
  plot(pfitHClust, labels = label)
  
  # If k is specified, draw rectangles to frame clusters in the dendrogram.
  if (is.na(k) == FALSE) {
    rect.hclust(pfitHClust, k = k)
  }
}
```

Now, plot dendrograms for different hierarchical clustering methods, here we use:

- "ward.D"
- "ward.D2"
- "complete"
- "average"

```{r hclust_04}
plot_hclust(hclust(d_euclidean, method = "ward.D"))
plot_hclust(hclust(d_euclidean, method = "ward.D2"))
plot_hclust(hclust(d_euclidean, method = "complete"))
plot_hclust(hclust(d_euclidean, method = "average"))
```

Plot dendrograms for "ward.D" by specifying the number of clusters, $k$ as 4 and 5 respectively.
```{r hclust_05}
plot_hclust(hclust(d_euclidean, method = "ward.D"), k = 4)
plot_hclust(hclust(d_euclidean, method = "ward.D"), k = 5)
```

### Running `clusterboot` for Hierarchical Clustering

Import "fpc" library as we will use the `clusterboot()` function to examine cluster stability.
```{r hclust_06, message=FALSE, warning=FALSE}
library("fpc")
```

Set seed value.
```{r hclust_07}
set.seed(123)
```

**Helper function #3**: Print selected results from `clusterboot`
```{r hclust_08}
display.cboot.result <- function(cboot.result) {
  summary(cboot.result)
  
  groups <- cboot.result$result$partition
  print_clusters(groups, kbest.p)
  
  print("Vector of cluster stabilities")
  print(cboot.result$bootmean)
  
  print("How many times each cluster was dissolved?")
  print(cboot.result$bootbrd)
}
```

Running `clusterboots` for **4 clusters** using 4 different clustering methods respectively.

*Only showing results for "ward.D" because the output will become very long if the complete set of results is shown.*
```{r hclust_09}
kbest.p <- 4

# Note: invisible(capture.output()) is used to suppress output
# ward.D
invisible(capture.output(cboot.hclust <- clusterboot(corona_scale, 
                                                     clustermethod = hclustCBI, 
                                                     method = "ward.D", 
                                                     k = kbest.p) ))
display.cboot.result(cboot.hclust)
```

```{r hclust_10, include=FALSE}
# ward.D2
invisible(capture.output(cboot.hclust <- clusterboot(corona_scale,
                                                     clustermethod = hclustCBI,
                                                     method = "ward.D2",
                                                     k = kbest.p) ))
display.cboot.result(cboot.hclust)

# complete
invisible(capture.output(cboot.hclust <- clusterboot(corona_scale,
                                                     clustermethod = hclustCBI,
                                                     method = "complete",
                                                     k = kbest.p) ))
display.cboot.result(cboot.hclust)

# average
invisible(capture.output(cboot.hclust <- clusterboot(corona_scale,
                                                     clustermethod = hclustCBI,
                                                     method = "average",
                                                     k = kbest.p) ))
display.cboot.result(cboot.hclust)
```

Running `clusterboots` for **5 clusters** using 4 different clustering methods respectively.
```{r hclust_11}
# 5 clusters
kbest.p <- 5

# ward.D
invisible(capture.output(cboot.hclust <- clusterboot(corona_scale, 
                                                     clustermethod = hclustCBI, 
                                                     method = "ward.D", 
                                                     k = kbest.p) ))
display.cboot.result(cboot.hclust)
```

```{r hclust_12, include=FALSE}
# ward.D2
invisible(capture.output(cboot.hclust <- clusterboot(corona_scale,
                                                     clustermethod = hclustCBI,
                                                     method = "ward.D2",
                                                     k = kbest.p) ))
display.cboot.result(cboot.hclust)

# complete
invisible(capture.output(cboot.hclust <- clusterboot(corona_scale,
                                                     clustermethod = hclustCBI,
                                                     method = "complete",
                                                     k = kbest.p) ))
display.cboot.result(cboot.hclust)

# average
invisible(capture.output(cboot.hclust <- clusterboot(corona_scale,
                                                     clustermethod = hclustCBI,
                                                     method = "average",
                                                     k = kbest.p) ))
display.cboot.result(cboot.hclust)
```

---

### Optimal k

```{r hclust_opt_k_01, include=FALSE, message=FALSE, warning=FALSE}
# Import "reshape2" library as we will use the `melt()` function from this library.
library(reshape2)
```

```{r hclust_opt_k_02, include=FALSE}
# Helper function #4: Calculate Euclidean distance between two vectors
sqr_edist <- function(x, y) { sum((x-y)^2) }
```

```{r hclust_opt_k_03, include=FALSE}
# Helper function #5: Calculate WSS of a cluster
wss.cluster <- function(clustermat) {
  # Calculate the centroid of the cluster
  c0 <- apply(clustermat, 2, FUN=mean)   
  # Calculate the sum of Euclidean distances between pairs of data points
  # and the cluster, c0
  sum(apply(clustermat, 1, FUN = function(row) { sqr_edist(row,c0) } ))  
}
```

```{r hclust_opt_k_04, include=FALSE}
# Helper function #6: Calculate total WSS for all clusters
wss.total <- function(dmatrix, labels) {                          
  wsstot <- 0    
  k <- length(unique(labels))
  
  for(i in 1:k)
    # Extract each cluster, calculate the cluster's WSS
    # and add up all the values
    wsstot <- wsstot + wss.cluster(subset(dmatrix, labels == i))
  
  wsstot
}
```

```{r hclust_opt_k_05, include=FALSE}
# Helper function #7: Calculate total sum of squares
totss <- function(dmatrix) {                       
  grandmean <- apply(dmatrix, 2, FUN=mean)    
  sum(apply(dmatrix, 1, FUN = function(row) { sqr_edist(row, grandmean) } ))
}
```

```{r hclust_opt_k_06, include=FALSE}
# Helper function #8: Calculate CH criterion for "kmeans" or "hclust".
ch_criterion <- function(dmatrix, kmax, method = "kmeans", ch.method = "ward.D2") {           
  if (!(method %in% c("kmeans", "hclust"))) {
    stop("method must be one of c('kmeans', 'hclust')")
  }
  
  npts <- dim(dmatrix)[1]  # number of rows.      
  
  # Total sum of squares is independent of the clustering
  totss <- totss(dmatrix)
  
  wss <- numeric(kmax)    
  crit <- numeric(kmax)    
  
  # Calculate WSS for k = 1 (which is the total sums of squares)
  wss[1] <- (npts - 1) * sum(apply(dmatrix, 2, var))
  
  # Calculate WSS for k from 2 to kmax
  for (k in 2:kmax) {
    if (method=="kmeans") {
      # kmeans() returns the total WSS as one of its outputs
      clustering <- kmeans(dmatrix, k, nstart = 10, iter.max = 100)        
      wss[k] <- clustering$tot.withinss
    }
    else {
      # for hclust(), calculate total wss by hand
      d <- dist(dmatrix, method = "euclidean")        
      pfit <- hclust(d, method = ch.method)
      # pfit <- hclust(d, method = "single")
      labels <- cutree(pfit, k = k)        
      wss[k] <- wss.total(dmatrix, labels)      
    }    
  }
  
  # Calculate BSS for k from 1 to kmax
  bss <- totss - wss   
  
  # NOrmalize BSS by k-1
  crit.num <- bss/(0:(kmax-1))
  
  # Normalize WSS by npts-k
  crit.denom <- wss/(npts - 1:kmax)   
  
  # Return a vector of CH indices and of WSS for k from 1 to kmax
  # Also return total sum of squares
  list(crit = crit.num/crit.denom, wss = wss, totss = totss)
}
```


```{r hclust_opt_k_07, include=FALSE}
# Helper function #9.
plot_ch_criterion <- function(pmat, kmax = 10, method = "hclust", hClustMethod = "ward.D2") {
  clustcrit <- ch_criterion(pmat, kmax, method = method, ch.method = hClustMethod)
  
  print(clustcrit$crit)
  print(clustcrit$wss)
  
  # Create a data frame with number of clusters, the scaled CH criterion 
  # and the scaled WSS criterion (enable plotting on the same graph)
  critframe <- data.frame(k = 1:kmax, 
                          ch = scale(clustcrit$crit), 
                          wss = scale(clustcrit$wss))
  
  # melt() function: put the data frame in a shape suitable for ggplot
  critframe <- melt(critframe, id.vars = c("k"), 
                    variable.name="measure", 
                    value.name="score")
  
  plottitle <- paste0("CH vs WSS: ", hClustMethod)
  print(plottitle)
  
  # Plot both CH and WSS
  ggplot(critframe, aes(x = k, y = score, color = measure)) + 
    geom_point(aes(shape = measure)) + 
    geom_line(aes(linetype = measure)) + 
    scale_x_continuous(breaks = 1:kmax, labels = 1:kmax) +
    ggtitle(plottitle)
}
```

Plot CH and WSS criteria on the same graph to find the optimal $k$.

```{r hclust_opt_k_08, echo=FALSE, results='hide', warning=FALSE}
plot_ch_criterion(corona_scale, kmax = 10, hClustMethod = "ward.D")
plot_ch_criterion(corona_scale, kmax = 10, hClustMethod = "ward.D2")
plot_ch_criterion(corona_scale, kmax = 10, hClustMethod = "complete")
plot_ch_criterion(corona_scale, kmax = 10, hClustMethod = "average")
```

### Plot Distribution of Clusters using PCA

```{r hclust_pca_01, include=FALSE}
# Helper function #10
plot_hclust_pc <- function(pmat, nComp, groups, subject, method.name = NA) {
  princ <- prcomp(pmat)
  
  project <- predict(princ, newdata = pmat)[, 1:nComp]
  
  project.plus <- cbind(as.data.frame(project), 
                        cluster = as.factor(groups), 
                        subject = subject)
  
  plottitle <- ifelse(!is.na(method.name), paste0("Clusters: ", method.name), "Clusters")
  
  plotobj <- ggplot(project.plus, aes(x = PC1, y = PC2, color = cluster)) + 
    geom_point(aes(shape = cluster)) + 
    geom_text(aes(label = subject), 
              hjust = 0, 
              vjust = 1, 
              check_overlap = TRUE,
              show.legend = FALSE) +
    ggtitle(plottitle)
  
  list(plotobj = plotobj, project = project, princ = princ)
}
```

Visualize clusters for $k$ = 4.

```{r hclust_pca_02, echo=FALSE, results='hide'}
# k = 4
num.k <- 4
pMethod <- "ward.D"

pfit2 <- hclust(d_euclidean, method = pMethod)
groups <- cutree(pfit2, k = num.k)

# Print list of virus for each cluster
print_clusters(groups, num.k)

# Plot data with respect to each cluster
corona_orig <- corona[,vars.to.use]

# Plot 
pc <- plot_hclust_pc(corona_orig, 2, groups, corona$X.subject.acc.ver., method.name = pMethod)
pc$plotobj


```


Visualize clusters for $k$ = 5.

```{r hclust_pca_03, echo=FALSE, results='hide'}
# k = 5
pMethod <- "ward.D"

pfit2 <- hclust(d_euclidean, method = pMethod)
groups <- cutree(pfit2, k = 5)

# Print list of virus for each cluster
print_clusters(groups, num.k)

# Plot data with respect to each cluster
corona_orig <- corona[,vars.to.use]

pc <- plot_hclust_pc(corona_orig, 2, groups, corona$X.subject.acc.ver., method.name = pMethod)
pc$plotobj
```

---

## K-Means Clustering

### View Optimal $k$

```{r kmeans_opt_k_01, include=FALSE, message=FALSE, warning=FALSE}
library(animation)
```

Generate principal components using PCA.

```{r kmeans_opt_k_02}
## PCA Features Extraction
nComp <- 2
corona <- corona.hclust[, vars.to.use]
pca <- predict(prcomp(corona), newdata = corona)[,1:nComp] 
```

Plot WSS criterion on the graph to identify the "elbow" - the optimal $k$.

WSS curve for the **unscaled** (i.e. the original) dataset.

```{r kmeans_opt_k_03, echo=FALSE}
##View optimal K (unscaled data set)
set.seed(123)
kmean_withinss <- function(k) {
  cluster <- kmeans(corona, k, iter.max = 100, nstart = 10)
  return (cluster$tot.withinss)
}

max_k <-10
wss <- sapply(2:max_k, kmean_withinss)
elbow <-data.frame(2:max_k, wss)

# Plot the graph with gglop
ggplot(elbow, aes(x = X2.max_k, y = wss)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(1, 20, by = 1))
```

WSS curve for the **scaled** dataset.

```{r kmeans_opt_k_04, echo=FALSE}
##View optimal K (scaled data set)
set.seed(123)
kmean_withinss <- function(k) {
  cluster <- kmeans(corona_scale, k, iter.max = 100, nstart = 10)
  return (cluster$tot.withinss)
}
max_k <-10
wss <- sapply(2:max_k, kmean_withinss)
elbow <-data.frame(2:max_k, wss)

# Plot the graph with gglop
ggplot(elbow, aes(x = X2.max_k, y = wss)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(1, 20, by = 1))
```


### Details of the K-means Clusters

Inspect details for the best k-means clusters, such as cluster labels, centroids and more.

```{r kmeans_opt_k_05}
corona_cluster_1 <- kmeans(corona, 5)

# Cluster labels: which cluster does each sample is assigned to?
corona_cluster_1$cluster

# The average values of the centroids (centers) for each cluster.
corona_cluster_1$centers

# How many samples in each cluster?
corona_cluster_1$size	
```

### Different Stages of K-means Clustering Algorithm

Plot different stages of the K-means clustering algorithm.

```{r kmeans_opt_k_06, echo=FALSE}
kmeans.ani(corona, 5)
```

---

## Fuzzy Clustering

### View Optimal $k$

```{r fuzzy_opt_k_01, include=FALSE, message=FALSE, warning=FALSE}
library(e1071) #library for Cmean Fuzzy Clustering

set.seed(123)
```


```{r fuzzy_opt_k_02}
# Helper function
plot_wss_fuzzy <- function(df, distance.method) {
  cmean_withinerror_fuzzy <- function(k) {
    cluster <- cmeans(df, k, iter.max = 100, dist = distance.method)
    return (cluster$withinerror)
  }
  
  max_k <-10
  wse <- sapply(2:max_k, cmean_withinerror_fuzzy)
  elbowfuzzy <-data.frame(2:max_k, wse)
  
  # Plot the graph with ggplot
  ggplot(elbowfuzzy, aes(x = X2.max_k, y = wse)) +
    geom_point() +
    geom_line() +
    scale_x_continuous(breaks = seq(1, 20, by = 1))
}
```


```{r fuzzy_opt_k_03}
# View optimal K (unscaled dataset) - Euclidean distance
plot_wss_fuzzy(corona, "euclidean")

# View optimal K (unscaled dataset) - Manhattan distance
plot_wss_fuzzy(corona, "manhattan")

# View optimal K (scaled dataset) - Euclidean distance
plot_wss_fuzzy(corona_scale, "euclidean")

# View optimal K (scaled dataset) - Manhattan distance
plot_wss_fuzzy(corona_scale, "manhattan")
```


### Details of the Fuzzy Clusters

Inspect details for the best Fuzzy clusters, such as cluster labels, centroids and more.

```{r fuzzy_opt_k_04}
corona_cluster_fuzzy <-cmeans(corona_scale, 5, dist = "euclidean")

# How many samples in each cluster?
corona_cluster_fuzzy$size

# Cluster labels: which cluster does each sample is assigned to?
corona_cluster_fuzzy$cluster
```

```{r fuzzy_opt_k_05}
clusterlabelscale <- as.data.frame(corona_cluster_fuzzy$cluster)
corona_label <- cbind(corona, clusterlabelscale)

cluster1 <- subset(corona_label, corona_cluster_fuzzy$cluster=='1')
cluster2 <- subset(corona_label, corona_cluster_fuzzy$cluster=='2')
cluster3 <- subset(corona_label, corona_cluster_fuzzy$cluster=='3')
cluster4 <- subset(corona_label, corona_cluster_fuzzy$cluster=='4')
cluster5 <- subset(corona_label, corona_cluster_fuzzy$cluster=='5')
```

Transform the mean values of each numerical attribute (the centroid of each cluster) into data frame.

```{r fuzzy_opt_k_06}
as.data.frame(colMeans(cluster1[sapply(cluster1, is.numeric)]))
as.data.frame(colMeans(cluster2[sapply(cluster2, is.numeric)]))
as.data.frame(colMeans(cluster3[sapply(cluster3, is.numeric)]))
as.data.frame(colMeans(cluster4[sapply(cluster4, is.numeric)]))
as.data.frame(colMeans(cluster5[sapply(cluster5, is.numeric)]))
```


### Plot Fuzzy Clustering Result

Visualize clusters for each sample trsnaformed into 2 principal components.

```{r fuzzy_opt_k_07, echo=FALSE, results='hide'}
princ <- prcomp(corona_scale)
nComp <- 2
corona_pca <- predict(princ, newdata=corona_scale)[,1:nComp]
as.data.frame(corona_pca)

corona_cluster_fuzzypca <- cmeans(corona_pca, 5, dist = "euclidean")

library(cluster)
clusplot(corona_pca, corona_cluster_fuzzypca$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

